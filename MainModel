# Import Libs
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import shap as shap

from sklearn.svm import SVC
from imblearn.over_sampling import BorderlineSMOTE
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedGroupKFold
from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, r2_score, make_scorer, recall_score, f1_score
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from imblearn.over_sampling import RandomOverSampler
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest
from sklearn import metrics
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from imblearn.pipeline import Pipeline

# Import data here
#filepath = ;
df = pd.read_csv(filepath)

# df.info()
# df.describe()
# df.head(5)

# Drop name - not needed, string to float errors
df['name_prefix'] = df['name'].str[:12]
name_prefix = df['name_prefix']
df = df.drop('name', axis=1)
df = df.drop('name_prefix', axis=1)

# Filter out highly correlated features
columns = list(df.columns)
for col in columns:
    if col == 'status':
        continue

    filtered_columns = [col]
    for col1 in df.columns:
        if ((col == col1) | (col == 'status')):
            continue

        val = df[col].corr(df[col1])

        if val > 0.7:
            # If the correlation between the two features is more than 0.7 remove
            columns.remove(col1)
            continue
        else:
            filtered_columns.append(col1)
    # After each iteration filter out the columns which are not highly correlated features.
    df = df[filtered_columns]


# Investigate data imbalance
x = df['status'].value_counts()
plt.pie(x.values, labels=x.index, autopct='%1.1f%%')
plt.show()

# Split data into training, validation, and groups
features = df.drop(['status'], axis=1)
target = df['status']
groups = name_prefix

# Plot Heatmap
# Compute the correlation matrix
corr_matrix = features.corr()

# Plot the heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap='coolwarm', square=True, cbar_kws={"shrink": 0.8})
plt.title('Feature Correlation Heatmap')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.show()


# Stratified GroupKFold
sgkf = StratifiedGroupKFold(n_splits=3)

# Set up the recall or f1 scorer for the minority class (status==0)
recall_minority = make_scorer(recall_score, pos_label=0)
f1_minority = make_scorer(f1_score, pos_label=1)


# RF Model
print("Random Forest")
print("")

# Define the pipeline steps
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=0.95)),
    ('smote', BorderlineSMOTE(kind='borderline-1', random_state=42)),
    ('clf', RandomForestClassifier(random_state=42))
])

# Define the hyperparameter grid for RandomForest
param_grid_rf = {
    'clf__n_estimators': [100, 200, 300],
    'clf__max_depth': [5, 7, 10, 12]
}

# Initialize GridSearchCV with StratifiedGroupKFold cross-validation
grid_search_rf = GridSearchCV(
    pipeline,
    param_grid=param_grid_rf,
    scoring=recall_minority,
    cv=sgkf.split(features, target, groups),
    n_jobs=-1
)

# Fit the model using the grid search
grid_search_rf.fit(features, target)

# Print out the best parameters and the best score
print(f"Best Hyperparameters: {grid_search_rf.best_params_}")
print(f"Best Recall Score for Minority Class: {grid_search_rf.best_score_:.3f}")

# Get the best model from grid search
best_model = grid_search_rf.best_estimator_

# Evaluate the model
y_pred = best_model.predict(features)

# Evaluation metrics
acc = accuracy_score(target, y_pred)
roc = roc_auc_score(target, y_pred)

print(f"Accuracy: {acc:.3f} | ROC AUC: {roc:.3f}")
print(classification_report(target, y_pred))

# Plot the confusion matrix
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
cm = confusion_matrix(target, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model.classes_)
disp.plot(cmap='Blues')
plt.title(f"Confusion Matrix (Random Forest)")
plt.show()


# LR Model
print("Logistic Regression")
print("")

# Define the pipeline steps for Logistic Regression
pipeline_logreg = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=0.95)),
    ('smote',  BorderlineSMOTE(kind='borderline-1', random_state=42)),
    ('clf', LogisticRegression(solver='liblinear', max_iter=1000))
])

# Define the recall scorer for the minority class
recall_minority = make_scorer(recall_score)

# Define the hyperparameter grid for Logistic Regression
param_grid_logreg = {
    'clf__C': [0.01, 0.1, 1, 10],
    'clf__penalty': ['l1', 'l2'],
    'clf__solver': ['liblinear', 'saga']
}

# Grid search with cross-validation
grid_search_logreg = GridSearchCV(
    pipeline_logreg,
    param_grid=param_grid_logreg,
    scoring=f1_minority,
    cv=sgkf.split(features, target, groups),
    n_jobs=-1
)

# Fit the model using the grid search
grid_search_logreg.fit(features, target)

# Print out the best parameters and the best score
print(f"Best Hyperparameters: {grid_search_logreg.best_params_}")
print(f"Best Recall Score for Minority Class: {grid_search_logreg.best_score_:.3f}")

# Get the best model from grid search
best_model_lr = grid_search_logreg.best_estimator_

# Evaluate the model
y_pred_lr = best_model_lr.predict(features)

# Evaluation metrics
acc = accuracy_score(target, y_pred_lr)
roc = roc_auc_score(target, y_pred_lr)

print(f"Accuracy: {acc:.3f} | ROC AUC: {roc:.3f}")
print(classification_report(target, y_pred_lr))

# Plot the confusion matrix
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
cm = confusion_matrix(target, y_pred_lr)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model_lr.classes_)
disp.plot(cmap='Purples')
plt.title(f"Confusion Matrix (Logistic Regression)")
plt.show()


# SVC Model
print("State Vector Classifier")
print("")

# Define the hyperparameter grid for SVC
pipeline_svc = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=0.95)),
    ('smote', BorderlineSMOTE(random_state=42)),
    ('clf', SVC(kernel='rbf', probability=True, class_weight='balanced'))
])

# Define the hyperparameter grid for SVC
param_grid_svc = {
    'clf__C': [0.1, 1, 10],
    'clf__gamma': ['scale', 'auto'],
    'clf__kernel': ['rbf', 'poly']
}

# Initialize GridSearchCV with StratifiedGroupKFold cross-validation
grid_search_svc = GridSearchCV(
    pipeline_svc,
    param_grid=param_grid_svc,
    scoring=f1_minority,  # Use recall for the minority class
    cv=sgkf.split(features, target, groups),
    n_jobs=-1
)

# Fit the model using the grid search
grid_search_svc.fit(features, target)

# Print out the best parameters and the best score
print(f"Best Hyperparameters: {grid_search_svc.best_params_}")
print(f"Best Recall Score for Minority Class: {grid_search_svc.best_score_:.3f}")

# Get the best model from grid search
best_model_svc = grid_search_svc.best_estimator_

# Evaluate the model
y_pred_svc = best_model_svc.predict(features)

# Evaluation metrics
acc = accuracy_score(target, y_pred_svc)
roc = roc_auc_score(target, y_pred_svc)

print(f"Accuracy: {acc:.3f} | ROC AUC: {roc:.3f}")
print(classification_report(target, y_pred_svc))

# Plot the confusion matrix
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
cm = confusion_matrix(target, y_pred_svc)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model_svc.classes_)
disp.plot(cmap='Reds')
plt.title(f"Confusion Matrix (State Vector Classifier)")
plt.show()
